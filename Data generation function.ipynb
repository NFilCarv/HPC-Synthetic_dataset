{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b0120f",
   "metadata": {},
   "source": [
    "# Data Generation Process\n",
    "From HPC-ODA dataset we use: Cpu cycles, Instructions, Cache misses, Branch Instructions, Branch Misses and Power consumption. \n",
    "\n",
    "From PM100 (a dataset generated from the M100 dataset) we use: Cores Allocated, Memory Allocated, Gpu's Allocated, Time and Power Consumption.\n",
    "\n",
    "Using relations between each of the HPC measures and the Power measure we built a percentace relation in order to then use those measure with the PM100 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd491c0",
   "metadata": {},
   "source": [
    "### Upload HPC-ODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db949f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HPC_ODA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_path = 'C://Users/Nuno/responses.csv'\n",
    "\n",
    "\n",
    "power_data = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/responses/responses.csv', skiprows=1, usecols=[1], header=None, names=['y'])\n",
    "cache_misses_data = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.cache-misses.csv', skiprows=1, usecols=[1], header=None, names=['y'])\n",
    "branch_instructions_data = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.branch-instructions.csv', skiprows=1, usecols=[1], header=None, names=['y']) ##These are branch instructions.\n",
    "branch_misses_data = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.branch-misses.csv', skiprows=1, usecols=[1], header=None, names=['y']) ##These are branch instructions.\n",
    "cpu_cycles_data = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.cpu-cycles.csv', skiprows=1, usecols=[1], header=None, names=['y'])\n",
    "instructions_data = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.instructions.csv', skiprows=1, usecols=[1], header=None, names=['y']) ##These are instructions.\n",
    "\n",
    "\n",
    "power = power_data['y'].values\n",
    "cache_misses = cache_misses_data['y'].values\n",
    "cache_misses = cache_misses[1:-1]\n",
    "instructions = instructions_data['y'].values\n",
    "instructions = instructions[1:-1]\n",
    "cpu_cycles = cpu_cycles_data['y'].values\n",
    "cpu_cycles = cpu_cycles[1:-1]\n",
    "branch_instructions = branch_instructions_data['y'].values \n",
    "branch_instructions = branch_instructions[1:-1]\n",
    "branch_misses = branch_misses_data['y'].values\n",
    "branch_misses= branch_misses[1:-1]\n",
    "\n",
    "instpow = np.divide(instructions,power)\n",
    "cachepow = np.divide(cache_misses,power)\n",
    "cpupow = np.divide(cpu_cycles,power)\n",
    "brinspow = np.divide(branch_instructions,power)\n",
    "brcachepow = np.divide(branch_misses,power)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede35d6",
   "metadata": {},
   "source": [
    "### Upload PM100 (Completed only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158c2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the data from a parquet file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    \n",
    "    # Path to the PM100 data\n",
    "    DATA_PATH = \"C://Users/Nuno/job_table.parquet\"\n",
    "    \n",
    "    # Load the dataframe\n",
    "    df = load_data(DATA_PATH)\n",
    "    \n",
    "    # Filter out completed jobs\n",
    "    df_completed = df[df['job_state'] == 'COMPLETED'].copy()  # Use copy() to explicitly create a copy\n",
    "    \n",
    "    # Exit state pie plot\n",
    "    df_completed['job_state'].replace({\"OUT_OF_MEMORY\": \"OOM+NODE FAIL\", \"NODE_FAIL\": \"OOM+NODE FAIL\"}, inplace=True)\n",
    "\n",
    "    # Plot the duration of the completed jobs divided by exit state\n",
    "    # Convert runtime to minutes\n",
    "    ##df_completed['run_time'] = df_completed['run_time'].apply(lambda rt: round(int(rt/60), -2))\n",
    "\n",
    "    #df_completed['power_consumption'].to_csv('p1.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09aaa197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label (matched):\n",
    "    i = 0\n",
    "    for key in matched.keys():\n",
    "        matched_list = matched[key]\n",
    "        if len(matched_list) == 1:\n",
    "            matched_list[0].append(0)\n",
    "        elif len(matched_list) > 1:\n",
    "            for i in range(len(matched_list)):\n",
    "                matched_list[i].append(1)\n",
    "                \n",
    "    return (matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911f83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def aux(regular, irregular):\n",
    "    high_bound = 0.05 * regular\n",
    "    if high_bound < 2 :\n",
    "        high_bound = 3\n",
    "    random_values = []\n",
    "    \n",
    "    while sum(random_values) < regular:\n",
    "        random_values.append(np.random.randint(2, high_bound))\n",
    "        \n",
    "\n",
    "    if sum(random_values) > regular:\n",
    "        adjustment = sum(random_values) - regular\n",
    "        while adjustment > 0:        \n",
    "            max_index = random_values.index(max(random_values))\n",
    "            random_values[max_index] -= 1\n",
    "            adjustment -= 1\n",
    "    \n",
    "    array_of_ones = [1] * irregular\n",
    "    \n",
    "    random_values.extend(array_of_ones)\n",
    "    return random_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6913438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_tuples(l, noise, num, add=None):\n",
    "    matched_list = l\n",
    "\n",
    "    for i in range(num):\n",
    "        lis = [0,0,0] \n",
    "        for j in range(len(lis)):\n",
    "            n = noise[j]\n",
    "            low_bound = matched_list[0][j] - matched_list[0][j] * n\n",
    "            high_bound = matched_list[0][j] + matched_list[0][j] * n\n",
    "            to_add = round(np.random.uniform(low_bound, high_bound))\n",
    "            lis[j] = to_add\n",
    "        matched_list.append(lis)\n",
    "    \n",
    "    return matched_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c11619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(l, dist, noise):\n",
    "    matched_dic = l\n",
    "    dist = sorted(dist, reverse=True)\n",
    "    i = 0\n",
    "    keys_to_change = []\n",
    "    new_matched = []\n",
    "    \n",
    "    keys_to_delete = []\n",
    "    for key in matched_dic.keys():\n",
    "        matched_list = matched_dic[key]\n",
    "        if i < len(dist):\n",
    "            new = dist[i] - len(matched_list)\n",
    "            if new > 0:\n",
    "                matched_list = generate_tuples(matched_list, noise, new)\n",
    "\n",
    "            elif new < 0:\n",
    "                keys_to_change.append(key)\n",
    "                matched_list = matched_list[:new]\n",
    "                new_matched.append(matched_list)\n",
    "            \n",
    "\n",
    "            else :\n",
    "                matched_list = matched_list\n",
    "\n",
    "        \n",
    "        elif i >= len(dist):\n",
    "            keys_to_delete.append(key)\n",
    "            \n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "    j = 0\n",
    "    for key in keys_to_change:\n",
    "        matched_dic[key] = new_matched[j]\n",
    "        j = j + 1\n",
    "\n",
    "    for key_to_delete in keys_to_delete:\n",
    "        del matched_dic[key_to_delete]\n",
    "        \n",
    "\n",
    "    return matched_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975a096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_percentage(l1,l2,l3,num_values, percentage, noise):      \n",
    "    inst = l1\n",
    "    bi = l2\n",
    "    time = l3\n",
    "    total_values = num_values\n",
    "    p = percentage\n",
    "    ni = noise[0] \n",
    "    nbi = noise[1] \n",
    "    nt = noise[2] \n",
    "    \n",
    "    matched_inst, p_inst = check_percentage_individual(inst,total_values,ni)\n",
    "    matched_bi, p_bi = check_percentage_individual(bi,total_values,nbi)\n",
    "    matched_time,p_time = check_percentage_individual(time,total_values,nt)\n",
    "    \n",
    "    to_regular = round((percentage/100) * num_values)\n",
    "    \n",
    "    rest_to_regular = total_values - to_regular\n",
    "    \n",
    "    dist = aux(to_regular)\n",
    "\n",
    "    \n",
    "    inst, inst_2 = fit(matched_inst,dist,ni)\n",
    "    bi, bi_2 = fit(matched_bi,dist,nbi)\n",
    "    time, time_2 = fit(matched_time,dist,nt)\n",
    "\n",
    "    if len(inst_2) >= rest_to_regular:\n",
    "        excess = len(inst_2) - rest_to_regular\n",
    "        inst_2 = inst_2[excess:]\n",
    "    else:\n",
    "        to_add = rest_to_regular - len(inst_2)\n",
    "        to_fill = generate_instructions(to_add)\n",
    "        inst_2 = np.concatenate((inst_2,to_fill))\n",
    "    \n",
    "    if len(bi_2) >= rest_to_regular:\n",
    "        excess = len(bi_2) - rest_to_regular\n",
    "        bi_2 = bi_2[excess:]\n",
    "    else:\n",
    "        to_add = rest_to_regular - len(bi_2)\n",
    "        to_fill = generate_branch_instructions(to_add)\n",
    "        bi_2 = np.concatenate((bi_2,to_fill))\n",
    "    \n",
    "    if len(time_2) >= rest_to_regular:\n",
    "        excess = len(time_2) - rest_to_regular\n",
    "        time_2 = time_2[excess:]\n",
    "    else:\n",
    "        to_add = rest_to_regular - len(time_2)\n",
    "        to_fill = generate_time(to_add)\n",
    "        time_2 = np.concatenate((time_2,to_fill))\n",
    "        \n",
    "    \n",
    "    \n",
    "    treated = list(zip(inst,bi,time))\n",
    "    \n",
    "    non_treated = list(zip(inst_2,bi_2,time_2))\n",
    "    \n",
    "    adjuction = treated + non_treated\n",
    "    \n",
    "    lim = len(dist)\n",
    "    \n",
    "    out = final_adjust(adjuction, total_values, noise, percentage, lim)\n",
    "        \n",
    "    \n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1afe9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_adjust(l, num_values, noise, percentage, num):\n",
    "    total_values = num_values\n",
    "    ni = noise[0]\n",
    "    nbi = noise[1]\n",
    "    nt = noise[2]\n",
    "    lim = num\n",
    "    p = percentage\n",
    "    check_tasks = l\n",
    "    per_in = check_percentage(l,noise)\n",
    "    regulated = round((percentage/100) * num_values)\n",
    "    while per_in != p:\n",
    "        i = 0 \n",
    "        n1 = noise[0]\n",
    "        n2 = noise[1]\n",
    "        n3 = noise[2]\n",
    "        \n",
    "        for i in range(regulated , len(check_tasks)):\n",
    "            value = check_tasks[i]\n",
    "        \n",
    "            for j in range(0, len(check_tasks)):  \n",
    "                if (abs(check_tasks[j][0] - value[0]) <= n1 * value[0] and\n",
    "                    abs(check_tasks[j][1] - value[1]) <= n2 * value[1] and\n",
    "                    abs(check_tasks[j][2] - value[2]) <= n3 * value[2]) and (i != j):\n",
    "                    arr1 = generate_instructions(total_values)\n",
    "                    arr2 = generate_branch_instructions(total_values)\n",
    "                    arr3 = generate_time(total_values)\n",
    "                    random_inst = np.random.choice(arr1)\n",
    "                    random_bi = np.random.choice(arr2)\n",
    "                    random_time = np.random.choice(arr3)\n",
    "                    check_tasks[i][0] = random_inst\n",
    "                    check_tasks[i][1] = random_bi\n",
    "                    check_tasks[i][2] = random_time\n",
    "             \n",
    "            per_in = check_percentage(check_tasks,noise)\n",
    "            \n",
    "            \n",
    "    return check_tasks\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23b2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix(matched_dic, total_values, interval, noise):\n",
    "    matched = matched_dic\n",
    "    lent = calc_values(matched)\n",
    "        \n",
    "    last_keys = list(matched.keys())[interval:]\n",
    "    \n",
    "    if lent > total_values:\n",
    "        to_remove = lent - total_values\n",
    "        keys_to_remove = random.sample(last_keys, to_remove)\n",
    "        \n",
    "        for key in keys_to_remove:\n",
    "            del matched[key]\n",
    "    else:\n",
    "        while lent < total_values:\n",
    "            to_add = total_values-lent\n",
    "            matched = find_irr(matched,to_add,noise)\n",
    "            \n",
    "    return (matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a24f0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_irr(matched,to_add,noise):\n",
    "    instr = []\n",
    "    bi = []\n",
    "    tim = []\n",
    "    for key in matched.keys():\n",
    "        matched_list = matched[key]\n",
    "        instr.append(matched_list[0][0])\n",
    "        bi.append(matched_list[0][1])\n",
    "        tim.append(matched_list[0][2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f47a721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "def generate_values(num_values,noise, percentage = None):\n",
    "    total_values = (num_values)\n",
    "    power = generate_power(total_values)\n",
    "    inst = generate_instructions(total_values)\n",
    "    cpucy = generate_cpu_cycles(total_values)\n",
    "    cache_misses = generate_cache_misses(total_values)\n",
    "    bi = generate_branch_instructions(total_values)\n",
    "    bm = generate_branch_misses(total_values)\n",
    "    memory = generate_memory(total_values)\n",
    "    time = generate_time(total_values)\n",
    "    cores = generate_cores(total_values)\n",
    "    gpus = generate_gpus(total_values)\n",
    "    nt = (noise/100)\n",
    "    ni = np.random.uniform(0.01,nt/2)\n",
    "    nbi = np.random.uniform(0.01,nt/2)\n",
    "    n = [ni,nbi,nt]\n",
    "    tasks = [list(t) for t in zip(inst, bi, time)]\n",
    "    matched = create_dict(tasks,n)\n",
    "    \n",
    "    print (\"DONE\")\n",
    "    if percentage is None:\n",
    "        \n",
    "        data = {\n",
    "        'instructions': inst,\n",
    "        'branch_instructions': bi,\n",
    "        'time': time,\n",
    "        'cpu_cycles': cpucy,\n",
    "        'cache_misses': cache_misses,\n",
    "        'branch_misses': bm,\n",
    "        'memory': memory,\n",
    "        'gpus': gpus,\n",
    "        'cores': cores,\n",
    "        'power': power\n",
    "        }\n",
    "    \n",
    "        gd = pd.DataFrame(data)\n",
    "        gd.to_parquet('generated_values.parquet', index=False)\n",
    "        \n",
    "        per_out = calc_percentage(matched)\n",
    "        \n",
    "    ##DATA_ADJUSTMENT:\n",
    "    else:\n",
    "        p = percentage\n",
    "        regu = int(total_values * (percentage/100))\n",
    "        irregu = int(total_values - regu)\n",
    "        dist = aux(regu, irregu)\n",
    "\n",
    "        matched= fit(matched,dist,n) ###UPDATE THE FITTING FUNCTION FOR THE DICTIONARY \n",
    "        per_out = calc_percentage(matched)\n",
    "        ##matched = fix(matched, total_values,interval)\n",
    "        ##per_out = calc_percentage(matched)\n",
    "        \n",
    "        lent = calc_values(matched)\n",
    "        \n",
    "        matched = add_label(matched)\n",
    "        \n",
    "        print (lent)\n",
    "        print (matched)\n",
    "        all_arrays = []\n",
    "        for key, arrays in matched.items():\n",
    "            for array in arrays:\n",
    "                all_arrays.append(array)\n",
    "\n",
    "        \n",
    "        random.shuffle(all_arrays)\n",
    "        df = pd.DataFrame(all_arrays, columns=['inst', 'bi', 'time', \"label\"])\n",
    "\n",
    "        df.to_parquet('teste2.parquet', engine='pyarrow')\n",
    "        ##data = {\n",
    "        ##'instructions': inst,\n",
    "        ##'cpu_cycles': cpucy,\n",
    "        ##'cache_misses': cacmis,\n",
    "        ##'branch_instructions': bi,\n",
    "        ##'branch_misses': bm,\n",
    "        ##'memory': memory,\n",
    "        ##'gpus': gpus,\n",
    "        ##'cores': cores,\n",
    "        ##'time': time,\n",
    "        ##'power': power\n",
    "        ##}\n",
    "    \n",
    "        ##gd = pd.DataFrame(data)\n",
    "        ##gd.to_parquet('generated_values.parquet', index=False)\n",
    "    return per_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99953468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "print(generate_values(200000,10,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b166d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(l, noise):\n",
    "    check_tasks = l\n",
    "    matched_dict = {}\n",
    "    i = 0  # Initialize i\n",
    "    n1, n2, n3 = noise  # Unpack noise values\n",
    "    while i < len(check_tasks):\n",
    "        matched_list = [check_tasks[i]]  # Initialize matched_list with the current index i\n",
    "        value = check_tasks[i]\n",
    "        indices_to_remove = []\n",
    "\n",
    "        for j in range(i + 1, len(check_tasks)):\n",
    "            if (abs(check_tasks[j][0] - value[0]) <= n1 * value[0] and\n",
    "                    abs(check_tasks[j][1] - value[1]) <= n2 * value[1] and\n",
    "                    abs(check_tasks[j][2] - value[2]) <= n3 * value[2]) and (i != j):\n",
    "                matched_list.append(check_tasks[j])\n",
    "                indices_to_remove.append(j)\n",
    "\n",
    "        for idx in sorted(indices_to_remove, reverse=True):\n",
    "            del check_tasks[idx]\n",
    "\n",
    "        matched_dict[i] = matched_list\n",
    "        i += 1\n",
    "\n",
    "    sorted_matched_dict = {k: v for k, v in sorted(matched_dict.items(), key=lambda item: len(item[1]), reverse=True)}\n",
    "    return sorted_matched_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d24f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_values(matched_dic):\n",
    "    lent = sum(len(matched_list) for matched_list in matched_dic.values())\n",
    "    \n",
    "    return lent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bae70411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percentage (matched):\n",
    "    total = 0\n",
    "    reg = 0\n",
    "\n",
    "    for key, value in matched.items():\n",
    "        total += len(value)\n",
    "        if len(value) > 1:\n",
    "            reg += len(value)\n",
    "\n",
    "    per = ((reg / total) * 100)\n",
    "    \n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eddda29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_percentage_individual(l, num_values, noise):\n",
    "    matched = []\n",
    "    i = 0  \n",
    "\n",
    "    while i < len(l):\n",
    "        matched_list = [l[i]]  \n",
    "        value = l[i]\n",
    "        indices_to_remove = []  \n",
    "        for j in range(i + 1, len(l)):  \n",
    "            if (abs(l[j] - value) <= noise * value):\n",
    "                matched_list.append(l[j])\n",
    "                indices_to_remove.append(j)\n",
    "                \n",
    "        for idx in sorted(indices_to_remove, reverse=True):\n",
    "            del l[idx]\n",
    "        \n",
    "        if len(matched_list) > 0:\n",
    "            matched.append(matched_list)  \n",
    "        i += 1\n",
    "\n",
    "    total = 0\n",
    "    reg = 0\n",
    "\n",
    "    for value in matched:\n",
    "        total += len(value)\n",
    "        if len(value) > 1:\n",
    "            reg += len(value)\n",
    "    \n",
    "    per = ((reg / total) * 100)         \n",
    "    \n",
    "    return matched, per\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e55f5",
   "metadata": {},
   "source": [
    "### Generate Power:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e50333ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the data from a parquet file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the parquet file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    DATA_PATH = \"C://Users/Nuno/job_table.parquet\"\n",
    "\n",
    "    df = load_data(DATA_PATH)\n",
    "\n",
    "    df_completed = df[df['job_state'] == 'COMPLETED'].copy()  # Use copy() to explicitly create a copy\n",
    "\n",
    "    df_completed['job_state'].replace({\"OUT_OF_MEMORY\": \"OOM+NODE FAIL\", \"NODE_FAIL\": \"OOM+NODE FAIL\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f6f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_power(num_values):\n",
    "    lis = df_completed[\"power_consumption\"]\n",
    "    total_power = []\n",
    "\n",
    "    for i in range(len(lis)):\n",
    "        mean = np.mean(lis.iloc[i])\n",
    "        total_power.append(mean)\n",
    "\n",
    "    counts_pow, bin_edges_pow = np.histogram(total_power, bins='auto')\n",
    "\n",
    "    percentages_pow = [(count / len(total_power)) * 100 for count in counts_pow]\n",
    "\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_pow = [(p * total_counts / 100) for p in percentages_pow]\n",
    "\n",
    "    min_counts = [1 if count == 0 and total_counts > 0 else count for count in counts_pow]\n",
    "\n",
    "    total_counts -= sum(counts_pow) - sum(min_counts)\n",
    "\n",
    "    percentages_pow = [(count / sum(min_counts)) * 100 for count in min_counts]\n",
    "\n",
    "    pow1 = []\n",
    "\n",
    "    for count, (start, end) in zip(min_counts, zip(bin_edges_pow[:-1], bin_edges_pow[1:])):\n",
    "        values = np.random.randint(start, end, math.ceil(count))\n",
    "        pow1.extend(values)\n",
    "    \n",
    "    np.random.shuffle(pow1)\n",
    "\n",
    "\n",
    "    return pow1[:num_values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e6f58",
   "metadata": {},
   "source": [
    "### Generate relations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3643a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_instructions(num_values):\n",
    "    power = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/responses/responses.csv', usecols=[1], header=0, names=['y'])['y'].values\n",
    "    instructions = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.instructions.csv', usecols=[1], header=0, names=['y'])['y'].values[1:-1]\n",
    "\n",
    "    inspow = np.divide(instructions, power)\n",
    "\n",
    "    counts_ins, bin_edges_ins = np.histogram(inspow, bins='auto')\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages_inst = [(count / len(inspow)) * 100 for count in counts_ins]\n",
    "\n",
    "    # Calculate the counts based on the percentages\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_inst = [(p * total_counts / 100) for p in percentages_inst]\n",
    "\n",
    "    # Ensure that each bin contributes at least one value\n",
    "    min_counts = [1 if count == 0 and total_counts > 0 else count for count in counts_inst]\n",
    "\n",
    "    # Adjust total_counts\n",
    "    total_counts -= sum(counts_inst) - sum(min_counts)\n",
    "\n",
    "    # Recalculate percentages\n",
    "    percentages_inst = [(count / sum(min_counts)) * 100 for count in min_counts]\n",
    "\n",
    "    # Generate random values based on intervals and counts\n",
    "    inspow1 = []\n",
    "\n",
    "    for count, (start, end) in zip(min_counts, zip(bin_edges_ins[:-1], bin_edges_ins[1:])):\n",
    "        # Generate random values within each bin\n",
    "        values = np.random.uniform(start, end, math.ceil(count))\n",
    "        inspow1.extend(values)\n",
    "\n",
    "    # Shuffle the generated values to make them random\n",
    "    np.random.shuffle(inspow1)\n",
    "    \n",
    "    inspow1 = inspow1[:num_values]\n",
    "    power = generate_power(num_values)\n",
    "    inst = [round(inspow1[i] * power[i]) for i in range(num_values)]\n",
    "\n",
    "    return inst  # Trim to the requested number of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "115183da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_cache_misses(num_values):\n",
    "    power = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/responses/responses.csv', usecols=[1], header=0, names=['y'])['y'].values\n",
    "    cache_misses = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.cache-misses.csv', usecols=[1], header=0, names=['y'])['y'].values[1:-1]\n",
    "\n",
    "    cachepow = np.divide(cache_misses, power)\n",
    "\n",
    "    counts_cache, bin_edges_cache = np.histogram(cachepow, bins='auto')\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages_cache = [(count / len(cachepow)) * 100 for count in counts_cache]\n",
    "\n",
    "    # Calculate the counts based on the percentages\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_cache = [(p * total_counts / 100) for p in percentages_cache]\n",
    "\n",
    "    # Ensure that each bin contributes at least one value\n",
    "    min_counts = [1 if count == 0 and total_counts > 0 else count for count in counts_cache]\n",
    "\n",
    "    # Adjust total_counts\n",
    "    total_counts -= sum(counts_cache) - sum(min_counts)\n",
    "\n",
    "    # Recalculate percentages\n",
    "    percentages_cache = [(count / sum(min_counts)) * 100 for count in min_counts]\n",
    "\n",
    "    # Generate random values based on intervals and counts\n",
    "    cachepow1 = []\n",
    "\n",
    "    for count, (start, end) in zip(min_counts, zip(bin_edges_cache[:-1], bin_edges_cache[1:])):\n",
    "        # Generate random values within each bin\n",
    "        values = np.random.uniform(start, end, math.ceil(count))\n",
    "        cachepow1.extend(values)\n",
    "\n",
    "    # Shuffle the generated values to make them random\n",
    "    np.random.shuffle(cachepow1)\n",
    "    \n",
    "    cachepow1[:num_values]\n",
    "    power = generate_power(num_values)\n",
    "    cm = [round(cachepow1[i] * power[i]) for i in range(num_values)]\n",
    "\n",
    "    return cm # Trim to the requested number of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af3b5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_cpu_cycles(num_values):\n",
    "    power = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/responses/responses.csv', usecols=[1], header=0, names=['y'])['y'].values\n",
    "    cpu_cycles = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.cpu-cycles.csv', usecols=[1], header=0, names=['y'])['y'].values[1:-1]\n",
    "\n",
    "    cpupow = np.divide(cpu_cycles, power)\n",
    "\n",
    "    counts_cpu, bin_edges_cpu = np.histogram(cpupow, bins='auto')\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages_cpu = [(count / len(cpupow)) * 100 for count in counts_cpu]\n",
    "\n",
    "    # Calculate the counts based on the percentages\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_cpu = [(p * total_counts / 100) for p in percentages_cpu]\n",
    "\n",
    "    # Ensure that each bin contributes at least one value\n",
    "    min_counts = [1 if count == 0 and total_counts > 0 else count for count in counts_cpu]\n",
    "\n",
    "    # Adjust total_counts\n",
    "    total_counts -= sum(counts_cpu) - sum(min_counts)\n",
    "\n",
    "    # Recalculate percentages\n",
    "    percentages_cpu = [(count / sum(min_counts)) * 100 for count in min_counts]\n",
    "\n",
    "    # Generate random values based on intervals and counts\n",
    "    cpupow1 = []\n",
    "\n",
    "    for count, (start, end) in zip(min_counts, zip(bin_edges_cpu[:-1], bin_edges_cpu[1:])):\n",
    "        # Generate random values within each bin\n",
    "        values = np.random.uniform(start, end, math.ceil(count))\n",
    "        cpupow1.extend(values)\n",
    "\n",
    "    # Shuffle the generated values to make them random\n",
    "    np.random.shuffle(cpupow1)\n",
    "    \n",
    "    cpupow1[:num_values]\n",
    "    power = generate_power(num_values)\n",
    "    cpp= [round(cpupow1[i] * power[i]) for i in range(num_values)]\n",
    "\n",
    "    return cpp  # Trim to the requested number of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72908a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_branch_instructions(num_values):\n",
    "    power = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/responses/responses.csv', usecols=[1], header=0, names=['y'])['y'].values\n",
    "    branch_instructions = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.branch-instructions.csv', usecols=[1], header=0, names=['y'])['y'].values[1:-1]\n",
    "\n",
    "    bipow = np.divide(branch_instructions, power)\n",
    "\n",
    "    counts_bi, bin_edges_bi = np.histogram(bipow, bins='auto')\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages_bi = [(count / len(bipow)) * 100 for count in counts_bi]\n",
    "\n",
    "    # Calculate the counts based on the percentages\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_bi = [(p * total_counts / 100) for p in percentages_bi]\n",
    "\n",
    "    # Ensure that each bin contributes at least one value\n",
    "    min_counts = [1 if count == 0 and total_counts > 0 else count for count in counts_bi]\n",
    "\n",
    "    # Adjust total_counts\n",
    "    total_counts -= sum(counts_bi) - sum(min_counts)\n",
    "\n",
    "    # Recalculate percentages\n",
    "    percentages_bi = [(count / sum(min_counts)) * 100 for count in min_counts]\n",
    "\n",
    "    # Generate random values based on intervals and counts\n",
    "    bipow1 = []\n",
    "\n",
    "    for count, (start, end) in zip(min_counts, zip(bin_edges_bi[:-1], bin_edges_bi[1:])):\n",
    "        # Generate random values within each bin\n",
    "        values = np.random.uniform(start, end, math.ceil(count))\n",
    "        bipow1.extend(values)\n",
    "\n",
    "    # Shuffle the generated values to make them random\n",
    "    np.random.shuffle(bipow1)\n",
    "    bipow1 = bipow1[:num_values]\n",
    "    power = generate_power(num_values)\n",
    "    bi = [round(bipow1 [i] * power[i]) for i in range(num_values)]\n",
    "\n",
    "    return bi  # Trim to the requested number of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a60f049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_branch_misses(num_values):\n",
    "    power = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/responses/responses.csv', usecols=[1], header=0, names=['y'])['y'].values\n",
    "    branch_misses = pd.read_csv('C://Users/Nuno/HPC-ODA/power_prediction/sensors/node0.branch-instructions.csv', usecols=[1], header=0, names=['y'])['y'].values[1:-1]\n",
    "\n",
    "    bmpow = np.divide(branch_misses, power)\n",
    "\n",
    "    counts_bm, bin_edges_bm = np.histogram(bmpow, bins='auto')\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages_bm = [(count / len(bmpow)) * 100 for count in counts_bm]\n",
    "\n",
    "    # Calculate the counts based on the percentages\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_bm = [(p * total_counts / 100) for p in percentages_bm]\n",
    "\n",
    "    # Ensure that each bin contributes at least one value\n",
    "    min_counts = [1 if count == 0 and total_counts > 0 else count for count in counts_bm]\n",
    "\n",
    "    # Adjust total_counts\n",
    "    total_counts -= sum(counts_bm) - sum(min_counts)\n",
    "\n",
    "    # Recalculate percentages\n",
    "    percentages_bm = [(count / sum(min_counts)) * 100 for count in min_counts]\n",
    "\n",
    "    # Generate random values based on intervals and counts\n",
    "    bmpow1 = []\n",
    "\n",
    "    for count, (start, end) in zip(min_counts, zip(bin_edges_bm[:-1], bin_edges_bm[1:])):\n",
    "        # Generate random values within each bin\n",
    "        values = np.random.uniform(start, end, math.ceil(count))\n",
    "        bmpow1.extend(values)\n",
    "\n",
    "    # Shuffle the generated values to make them random\n",
    "    np.random.shuffle(bmpow1)\n",
    "    \n",
    "    bmpow1[:num_values]\n",
    "    power = generate_power(num_values)\n",
    "    bmp= [round(bmpow1[i] * power[i]) for i in range(num_values)]\n",
    "\n",
    "    # Trim to the requested number of values\n",
    "    return bmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebea674",
   "metadata": {},
   "source": [
    "### Generate Mixed Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e25de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_time(num_values):\n",
    "    time = np.array(df_completed[\"run_time\"])\n",
    "    \n",
    "    counts_time, bin_edges_time = np.histogram(time, bins='auto')\n",
    "\n",
    "    percentages_time = [(count / len(time)) * 100 for count in counts_time]\n",
    "\n",
    "    # Calculate the counts based on the percentages\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_time = [(p * total_counts / 100) for p in percentages_time]\n",
    "\n",
    "    # Ensure that each bin contributes at least one value\n",
    "    min_counts = [1 if count == 0 and total_counts > 0 else count for count in counts_time]\n",
    "\n",
    "    # Adjust total_counts\n",
    "    total_counts -= sum(counts_time) - sum(min_counts)\n",
    "\n",
    "    # Recalculate percentages\n",
    "    percentages_time = [(count / sum(min_counts)) * 100 for count in min_counts]\n",
    "\n",
    "    # Generate random values based on intervals and counts\n",
    "    time1 = []\n",
    "\n",
    "    for count, (start, end) in zip(min_counts, zip(bin_edges_time[:-1], bin_edges_time[1:])):\n",
    "        # Generate random values within each bin\n",
    "        values = np.random.randint(start, end, math.ceil(count))\n",
    "        time1.extend(values)\n",
    "\n",
    "    # Shuffle the generated values to make them random\n",
    "    np.random.shuffle(time1)\n",
    "\n",
    "    return time1[:num_values]  # Trim to the requested number of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60229366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_memory(num_values):\n",
    "    mem = np.array(df_completed[\"mem_alloc\"])\n",
    "    \n",
    "    counts_mem, bin_edges_mem = np.histogram(mem, bins='auto')\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentages_mem = [(count / len(mem)) * 100 for count in counts_mem]\n",
    "\n",
    "    # Calculate the counts based on the percentages\n",
    "    total_counts = num_values\n",
    "\n",
    "    counts_mem = [(p * total_counts / 100) for p in percentages_mem]\n",
    "\n",
    "    # Ensure that each bin contributes at least one value\n",
    "    while sum(counts_mem) < num_values:\n",
    "        counts_mem = [count + 1 for count in counts_mem]\n",
    "\n",
    "    # Generate random values based on intervals and counts\n",
    "    mem1 = []\n",
    "\n",
    "    for count, (start, end) in zip(counts_mem, zip(bin_edges_mem[:-1], bin_edges_mem[1:])):\n",
    "        # Generate random values within each bin\n",
    "        values = np.random.randint(start, end, math.ceil(count))\n",
    "        mem1.extend(values)\n",
    "\n",
    "    # Shuffle the generated values to make them random\n",
    "    np.random.shuffle(mem1)\n",
    "\n",
    "    return mem1[:num_values]  # Trim to the requested number of values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85d8d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def generate_memory1(num_values):\n",
    "    mem = np.array(df_completed[\"mem_alloc\"])\n",
    "    \n",
    "    counts_mem, bin_edges_mem = np.histogram(mem, bins='auto')\n",
    "    \n",
    "    percentages_mem = [(count / len(mem)) * 100 for count in counts_mem]\n",
    "    \n",
    "    \n",
    "    total_counts = num_values\n",
    "    \n",
    "    mem1 = []\n",
    "    i = 0\n",
    "\n",
    "    while (len(mem1) < num_values):\n",
    "        print (len(counts))\n",
    "        print (len(mem1))\n",
    "        if i >= total_counts:\n",
    "            for j in range(len(counts)):\n",
    "                percentages_mem[j] = (counts[j] / (sum(counts))) * 100\n",
    "            i = 0\n",
    "        counts = [(p * total_counts / 100) for p in percentages_mem]\n",
    "        for count, (start, end) in zip(counts, zip(bin_edges_mem[:-1], bin_edges_mem[1:])):\n",
    "            if count < 0:\n",
    "                count =0 \n",
    "            values = np.random.randint(start, end, int(count))\n",
    "            mem1.extend(values)\n",
    "            total_counts = total_counts - int(count)\n",
    "            count = count - (int(count))\n",
    "            i = i + 1\n",
    "            \n",
    "    np.random.shuffle(mem1)\n",
    "    \n",
    "    return mem1[:num_values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c0a7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_gpus(num_values):\n",
    "    gpus = np.array(df_completed[\"num_gpus_alloc\"])\n",
    "    total_counts = num_values\n",
    "    count_dict = {}\n",
    "    for num in gpus:\n",
    "        count_dict[num] = count_dict.get(num, 0) + 1\n",
    "\n",
    "    percentages_gpus = [(count / len(gpus)) * 100 for count in count_dict.values()]\n",
    "    counts_gpus = [int(p * total_counts / 100) for p in percentages_gpus]\n",
    "    \n",
    "    while sum(counts_gpus) < num_values:\n",
    "        counts_gpus = [count + 1 for count in counts_gpus]\n",
    "    \n",
    "    i = 0\n",
    "    gpus1 = []\n",
    "    for num, count in count_dict.items():\n",
    "        gpus1.extend([num] * counts_gpus[i])\n",
    "        i = i + 1\n",
    "\n",
    "    np.random.shuffle(gpus1)\n",
    "\n",
    "    return gpus1[:num_values]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebd29124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_cores(num_values):\n",
    "    cores = np.array(df_completed[\"num_cores_alloc\"])\n",
    "    total_counts = num_values\n",
    "    count_dict = {}\n",
    "    for num in cores:\n",
    "        count_dict[num] = count_dict.get(num, 0) + 1\n",
    "\n",
    "    percentages_cores = [(count / len(cores)) * 100 for count in count_dict.values()]\n",
    "    counts_cores = [int(p * total_counts / 100) for p in percentages_cores]\n",
    "    \n",
    "    while sum(counts_cores) < num_values:\n",
    "        counts_cores = [count + 1 for count in counts_cores]\n",
    "\n",
    "    cores1 = []\n",
    "    i = 0\n",
    "    for num, count in count_dict.items():\n",
    "        cores1.extend([num] * counts_cores[i])\n",
    "        i = i + 1\n",
    "\n",
    "    np.random.shuffle(cores1)\n",
    "\n",
    "    return cores1[:num_values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6880d",
   "metadata": {},
   "source": [
    "### How is this data generated:\n",
    "\n",
    "The power consumption, execution time, number of cores allocated, number of gpus allocated and memory allocated result from the distribution of data in the PM100 dataset which comes from the M100 Exadata. The data was analysed and the building the histograms of each of these parameters we extracted the probability of data points in certain intervals considering that these distributions did not come close to fit a known distribution.\n",
    "\n",
    "Using the HPC-ODA dataset, we extracted the relations (with the same steps as before) between instructions and power, cache misses and power, branch instructions and power, branch misses and power and cpu cycles and power with these distributions we generated random relations within it and multipled it by the value of power generated above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760693d",
   "metadata": {},
   "source": [
    "### Adding tasks that would be recognized as regular/periodic tasks\n",
    "\n",
    "In order to do this we use a task that is generated randomly, copy it and spread it a number of times through the dataset. This can be done for one or more tasks and the copy will not be exacly alike but instead just identical in order to recreate in a syntethical way a real regular enviroment of task. Still undecided on the criticallity of tasks and dependecies of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d375573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5125a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
